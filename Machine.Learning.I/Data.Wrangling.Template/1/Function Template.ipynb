{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Function for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic functions\n",
    "import pandas as pd\n",
    "import missingno as mano\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math as mt \n",
    "import statistics as st\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 1: Data read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function read data from excel and csv files to dataframe.\n",
    "# Input Arguments file: File name with path (Eg: 'D:/data/dataread.csv'), f_type: File Type (Eg: 'csv',default csv)\n",
    "def file_todataframe(file,f_type):\n",
    "    if f_type == 'csv':  \n",
    "        return pd.read_csv(file)\n",
    "    elif f_type == 'excel': \n",
    "        return pd.read_excel(file)\n",
    "    elif f_type == 'json': \n",
    "        return pd.read_json(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 2: Data Details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function display shape, data type, data near head and tail of given data fram.\n",
    "# Input Arguments df: dataframe, n: No f data points to display\n",
    "def df_details(d_f,n):\n",
    "    print('Data Types of Column: \\n',d_f.dtypes)\n",
    "    print('\\n Size of Datarame: ',d_f.shape)\n",
    "    print('\\n Top and bottom ',n,' rows: \\n')\n",
    "    display(d_f.head(n).append(d_f.tail(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 3: Droping Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops irrelevant columns\n",
    "# Input Arguments df: dataframe, col_del: Value or index array of column to delete (Eg: [1,3,5] or ['Names',Sales]), \n",
    "# typ: 1 for column index and 0 for column name in col_del\n",
    "def col_drop(d_f,col_del,typ):\n",
    "    if typ == 0:\n",
    "        d_f = d_f.drop(col_del,axis=1)\n",
    "    elif typ == 1:\n",
    "        d_f = d_f.drop(df.columns[col_del],axis=1)\n",
    "    return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 4: Droping Rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops rows with particular column values\n",
    "# Input Arguments df: dataframe, row_del: Delete if row with given value,\n",
    "# col_ref: Name of column to check for row values (Eg:['Names']), \n",
    "def row_drop(d_f,row_del,col_ref):\n",
    "    d_f = d_f.drop(d_f[d_f[col_ref] == row_del].index)\n",
    "    return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 5: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function give deatials for missing values in data\n",
    "# Input Arguments df: dataframe\n",
    "def miss_ch(d_f):\n",
    "    print('Available data with no nulls: ', d_f.dropna().shape[0])\n",
    "    display('Deatils of Null values column wise',d_f.isnull().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 6: Missing Values Analysis using Missing No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function give deatials for missing values in data\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# graph: Types of graph to display. (Eg: ['bar','matrix','heatmap','dendrogram'],['all'] default 'all')\n",
    "def miss_viz(d_f,col_int = 'all',graph = 'all'):\n",
    "    import missingno as mano\n",
    "    av_gp = ['bar','matrix','heatmap','dendrogram']\n",
    "    col_nam = d_f.columns\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    \n",
    "    if graph == 'all':\n",
    "        graph = av_gp\n",
    "    elif not all(i in av_gp for i in graph):\n",
    "        print(\"Invalid Graph type, select 'bar','matrix','heatmap','dendrogram'\")\n",
    "        return\n",
    "    \n",
    "    for gp in graph:\n",
    "        getattr(mano, gp)(d_f[col_int])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 7: Filling missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function fill missing values\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# metd: Types of graph to display. (Option: {0(float),‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, 'linear'} default None)\n",
    "def fill_miss(d_f,col_int = 'all',metd = None):\n",
    "    col_nam = d_f.columns\n",
    "    av_method = ['backfill', 'bfill', 'pad', 'ffill', 'linear']\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return d_f\n",
    "    \n",
    "    if metd == None:\n",
    "        return d_f\n",
    "    elif (type(metd) == int) | (type(metd) == float):\n",
    "        print('yes')\n",
    "        return d_f[col_int].fillna(metd)  \n",
    "    elif metd == 'linear':\n",
    "        return d_f[col_int].interpolate(method = 'linear') \n",
    "    elif  metd in av_method:\n",
    "        return d_f[col_int].fillna(method = 'ffill')\n",
    "    else:\n",
    "        print(\"Invalid fill type\")\n",
    "        return d_f\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 8: Numerical Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Numerical Data Analysis\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# func: Types of graph and function. (Option: {'distplot','boxpot','scatterplot','describe','normality'} default all)\n",
    "def data_num(d_f,col_int = 'all',func = 'all',scat = None):\n",
    "    from scipy.stats import shapiro\n",
    "    from statsmodels.graphics.gofplots import qqplot\n",
    "    av_func = ['hist','boxplot','scatter','describe','normality']\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    col_nam = d_f.select_dtypes(include=numerics).columns\n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    \n",
    "    #Checking Parameter available function\n",
    "    if func == 'all':\n",
    "        func = av_func\n",
    "    elif not all(i in av_gp for i in graph):\n",
    "        print(\"Invalid Graph type, select 'distplot','boxpot','scatterplot','describe','normality'\")\n",
    "        return\n",
    "    if scat is None:\n",
    "        scat = df.columns[0]\n",
    "    \n",
    "    \n",
    "    for fn in func:\n",
    "        if fn == 'describe':\n",
    "            display(\"Statictical Details\",df[col_int].describe())\n",
    "        else:\n",
    "            for col in col_int:\n",
    "                if fn == 'normality':\n",
    "                    #qqplot(df[col])\n",
    "                    print(\"Normality Test for: \",col)\n",
    "                    stat, p = shapiro(df[col])\n",
    "                    if p > 0.05:\n",
    "                        print('Sample looks Gaussian. Statistics=%.3f, p=%.3f'% (stat, p))\n",
    "                    else:\n",
    "                        print('Sample does not look Gaussian. Statistics=%.3f, p=%.3f'% (stat, p))\n",
    "                elif fn == 'scatter':\n",
    "                    display(col)\n",
    "                    plt.scatter(df[col],df[scat])  \n",
    "                    plt.show()\n",
    "                else:\n",
    "                    display(col)\n",
    "                    getattr(plt, fn)(df[col])   \n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 9: Categorical Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Categorical Data Analysis\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all')\n",
    "def data_cat(d_f,col_int = 'all',bar = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    col_nam = d_f.select_dtypes(include=['object','category']).columns\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    col_num = d_f.select_dtypes(include=numerics).columns   \n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "\n",
    "    if not ((bar in col_num) | (bar is None)):\n",
    "        print(\"Only numeric column for Bar\")\n",
    "        return \n",
    "    if not (bar is None):\n",
    "        for col in col_int:\n",
    "            plt.bar(d_f[col], d_f[bar])\n",
    "            plt.show()\n",
    "        return\n",
    "    for col in col_int:\n",
    "        d_f[col].value_counts().plot(kind='bar')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 10: Column Type Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Canging column type\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# dtyp: New data types of coloumn default int\n",
    "def col_dtype(d_f,col_int,dtyp = int):\n",
    "    d_f = deep_copy(d_f)\n",
    "    av_fun = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','int','float','str','category']\n",
    "    col_nam = d_f.columns\n",
    "    #Checking Parameter Column names\n",
    "    if not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    #Checking Parameter available function\n",
    "    if not dtyp in av_fun:\n",
    "        print(\"Invalid data type\")\n",
    "        return\n",
    "    \n",
    "    d_f[col_int] = d_f[col_int].astype(dtyp,errors='ignore')\n",
    "    return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 11: Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Canging column type\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# opr: Operation to be performed on coloumn values, val = [old value, new value]\n",
    "def col_opre(d_f,col_int = 'all',opr = None,val = None):\n",
    "    col_nam = d_f.select_dtypes(include=['object']).columns\n",
    "    av_opr = ['str_replace','rm_space','chg_value']\n",
    "    \n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "\n",
    "    if opr == 'str_replace':\n",
    "        d_f[col_int] = d_f[col_int].str.replace(val[0], val[1])\n",
    "        return d_f\n",
    "    elif opr == 'rm_space':\n",
    "        d_f[col_int] = d_f[col_int].str.replace(' ', '')\n",
    "        return d_f\n",
    "    elif opr == 'chg_value':\n",
    "        d_f[col_int] = d_f[col_int].replace(val[0], val[1])\n",
    "        return d_f\n",
    "    else:\n",
    "        print(\"Invalid Opeation\")\n",
    "        return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 12: Basis Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Performing numerical operations to column values\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'), \n",
    "# opr: Operation to be performed on coloumn values, val: value to apply on column\n",
    "def col_opre(d_f,col_int = 'all',opr = None,val = None):\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',]\n",
    "    col_num = d_f.select_dtypes(include=numerics).columns  \n",
    "\n",
    "\n",
    "    if opr == 'add':\n",
    "        d_f[col_int] = d_f[col_int]+val\n",
    "        return d_f\n",
    "    elif opr == 'sub':\n",
    "        d_f[col_int] = d_f[col_int]-val\n",
    "        return d_f\n",
    "    elif opr == 'mul':\n",
    "        d_f[col_int] = d_f[col_int]*val\n",
    "        return d_f\n",
    "    elif opr == 'div':\n",
    "        d_f[col_int] = d_f[col_int]/val\n",
    "        return d_f\n",
    "    else:\n",
    "        print(\"Invalid Opeation\")\n",
    "        return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 13: Deep Dataframe Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a deep copy\n",
    "# Input Arguments df: dataframe, Output data frame copy\n",
    "def deep_copy(d_f):\n",
    "    return d_f.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 14: Categorical to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for coding Categorical variable  to Numeric\n",
    "# Input Arguments df: dataframe, col_int: Columns of intrest (Eg: ['Sale','Customer'],['all'] default 'all'{must be string}), \n",
    "# coding_type: Coding type to apply{'label','binary','ordinal','onehot'} default,label, contain: For binary\n",
    "def cat_num(d_f,col_int,coding_type = 'label',contain=None):\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    col_nam = d_f.select_dtypes(include=['object']).columns\n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    \n",
    "    if coding_type == 'label':\n",
    "        d_f[col_int] = d_f[col_int].astype('category')\n",
    "        d_f[col_int] = d_f[col_int].cat.codes\n",
    "        return d_f\n",
    "    elif coding_type == 'binary':\n",
    "        d_f[col_int] = np.where(d_f[col_int].str.contains(contain), 1, 0)\n",
    "        return d_f\n",
    "    elif coding_type == 'ordinal':\n",
    "        ord_enc = OrdinalEncoder()\n",
    "        d_f[col_int] = ord_enc.fit_transform(d_f[[col_int]])\n",
    "        return d_f\n",
    "    elif coding_type == 'onehot':\n",
    "        oe_style = OneHotEncoder()\n",
    "        d_f = oe_style.fit_transform(d_f[[col_int]])\n",
    "        return d_f\n",
    "    else:\n",
    "        display(\"Invalid Ending Method\")\n",
    "        return d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 15: ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applyng Machine learning Models\n",
    "# Input Arguments df with last raw as labels, task: regression or classification, algo: Descion Tree or Random Forest\n",
    "def ml_algo(x,y,algo = 'decisiontree',task = 'Reg',n=3):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n",
    "    if task == 'Reg':\n",
    "        if algo == 'decisiontree':\n",
    "            model = DecisionTreeRegressor(max_depth=n)\n",
    "\n",
    "        elif algo == 'randomforest':\n",
    "            model = RandomForestRegressor(n_estimators = n, random_state = 42)\n",
    "            \n",
    "        elif algo == 'knn':\n",
    "            model = KNeighborsRegressor(n_neighbors=n)\n",
    "        else:\n",
    "            print(\"Invalid Algorithm\")\n",
    "            return  \n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r_2 = r2_score(y_test, y_pred)\n",
    "        r_2adj = r2_score(y_test, y_pred,multioutput='variance_weighted')\n",
    "        return mae,mse,r_2,r_2adj,model\n",
    "        \n",
    "    elif task == 'Class':    \n",
    "        if algo == 'decisiontree':\n",
    "            model = DecisionTreeClassifier(max_depth=n) \n",
    "        \n",
    "        elif algo == 'randomforest':\n",
    "            model=RandomForestClassifier(n_estimators=n)\n",
    "            \n",
    "        # n neighbour\n",
    "        elif algo == 'knn':\n",
    "            model = KNeighborsClassifier(n_neighbors=n)\n",
    "        else:\n",
    "            print(\"Invalid Algorithm\")\n",
    "            return\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        c_m =confusion_matrix(y_test, y_pred)\n",
    "        c_r = classification_report(y_test, y_pred)\n",
    "        acc_sc = accuracy_score(y_test, y_pred)\n",
    "        auc_roc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "        logloss = log_loss(y_test, y_pred)        \n",
    "        return c_m,c_r,acc_sc,auc_roc,logloss,model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 16: ANOVA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applyng Machine learning Models\n",
    "# Input Arguments df with last raw as labels, task: regression or classification, algo: Descion Tree or Random Forest\n",
    "def anova(d_f,col_int='all',col_main=None):\n",
    "    from scipy import stats\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',]\n",
    "    col_num = d_f.select_dtypes(include=numerics).columns \n",
    "    \n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    \n",
    "    for col in col_int:\n",
    "        model = ols(col_main+'~ C(Q(\"'+col+'\"))', data=d_f).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        print (\"\\nAnova =>\",col_main,\" - \",col)\n",
    "        display(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 17: Correlation Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function Correlation Heat Map\n",
    "# Input Arguments d_f: dataframe\n",
    "def corr_hmap(d_f):\n",
    "    import seaborn as sns\n",
    "    sns.set(rc={'figure.figsize':(15,8)})\n",
    "    corr = d_f.corr().dropna(1,'all').dropna(0,'all')\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=90,\n",
    "        horizontalalignment='right'\n",
    "    );\n",
    "    ax.set_yticklabels(\n",
    "        ax.get_yticklabels(),\n",
    "        rotation=0,\n",
    "        horizontalalignment='right'\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion 18: Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function scatter Plot\n",
    "# Input Arguments df: dataframe, col_int: Two columns of intrest (Eg: ['Sale','Customer'],['all']), \n",
    "def scatter_plot(d_f,col_int = None):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    col_nam = d_f.select_dtypes(include=numerics).columns\n",
    "    if (col_int is None) & (not all(i in col_nam for i in col_int)) & (len(col_int) != 2):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    x = d_f[col_int[0]]\n",
    "    y = d_f[col_int[1]]\n",
    "    xv = col_int[0]+' X- Value'\n",
    "    yv = col_int[1]+' Y- Value'\n",
    "    tit = col_int[0] + ' vs ' + col_int[1] +' Scatter plot'\n",
    "\n",
    "    plt.scatter(x, y)\n",
    "    plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "    plt.title(tit)\n",
    "    plt.xlabel(xv)\n",
    "    plt.ylabel(yv)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Anova, T-test, correlation heatmaps, normality tests\n",
    "Anova Correlation Heat Map and Normality already done (Compiled Once Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applyng Machine learning Models\n",
    "# Input Arguments df with last raw as labels, task: regression or classification, algo: Descion Tree or Random Forest\n",
    "def anova(d_f,col_int='all',col_main=None):\n",
    "    from scipy import stats\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',]\n",
    "    col_num = d_f.select_dtypes(include=numerics).columns \n",
    "    \n",
    "    #Checking Parameter Column names\n",
    "    if col_int == 'all':\n",
    "        col_int = col_nam\n",
    "    elif not all(i in col_nam for i in col_int):\n",
    "        print(\"Invalid column name\")\n",
    "        return \n",
    "    \n",
    "    for col in col_int:\n",
    "        model = ols(col_main+'~ C(Q(\"'+col+'\"))', data=d_f).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        print (\"\\nAnova =>\",col_main,\" - \",col)\n",
    "        display(anova_table)\n",
    "\n",
    "## T Test\n",
    "def t_test(d_f,col_ind=[1,2]):\n",
    "    data1 = d_f.iloc[:,col_ind[0]].values\n",
    "    data2 = d_f.iloc[:,col_ind[1]].values\n",
    "    # calculate means\n",
    "    mean1, mean2 = st.mean(data1), st.mean(data2)\n",
    "    # calculate sample standard deviations\n",
    "    std1, std2 = st.stdev(data1), st.stdev(data2)\n",
    "    # calculate standard errors\n",
    "    n1, n2 = len(data1), len(data2)\n",
    "    se1, se2 = std1/mt.sqrt(n1), std2/mt.sqrt(n2)\n",
    "    # standard error on the difference between the samples\n",
    "    sed = mt.sqrt(se1**2.0 + se2**2.0)\n",
    "    # calculate the t statistic\n",
    "    t_stat = (mean1 - mean2) / sed\n",
    "    print('T Test Statistics=%.3f' % (t_stat))\n",
    "\n",
    "## Normality Test\n",
    "def norm(d_f,col_ind=[1]):\n",
    "    for col in col_ind:\n",
    "        data = d_f.iloc[:,col]\n",
    "        stat, p = shapiro(data)\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "        # interpret\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            print('Sample looks Gaussian (fail to reject H0)')\n",
    "        else:\n",
    "            print('Sample does not look Gaussian (reject H0)')\n",
    "        data.hist()\n",
    "        \n",
    "# This function Correlation Heat Map\n",
    "# Input Arguments d_f: dataframe\n",
    "def corr_hmap(d_f):\n",
    "    import seaborn as sns\n",
    "    sns.set(rc={'figure.figsize':(15,8)})\n",
    "    corr = d_f.corr().dropna(1,'all').dropna(0,'all')\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=90,\n",
    "        horizontalalignment='right'\n",
    "    );\n",
    "    ax.set_yticklabels(\n",
    "        ax.get_yticklabels(),\n",
    "        rotation=0,\n",
    "        horizontalalignment='right'\n",
    "    );    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Classification and Regression Remaning Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applyng Machine learning Models\n",
    "# Input Arguments df with last raw as labels, task: regression or classification, algo: Descion Tree or Random Forest\n",
    "def ml_algo(X_train, X_test, y_train, y_test,algo = 'decisiontree',task = 'Reg',n=3):\n",
    "\n",
    "    if task == 'Reg':\n",
    "        if algo == 'decisiontree':\n",
    "            model = DecisionTreeRegressor(max_depth=n)\n",
    "\n",
    "        elif algo == 'randomforest':\n",
    "            model = RandomForestRegressor(n_estimators = n, random_state = 42)\n",
    "            \n",
    "        elif algo == 'knn':\n",
    "            model = KNeighborsRegressor(n_neighbors=n)\n",
    "            \n",
    "        elif algo == 'lreg':\n",
    "            model = LinearRegression()\n",
    "            \n",
    "        elif algo == 'svm':\n",
    "            model = SVR(kernel=\"rbf\")\n",
    "            \n",
    "        elif algo == 'gboost':\n",
    "            model = GradientBoostingRegressor()\n",
    "            \n",
    "        elif algo == 'adaboost':\n",
    "            model = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid Algorithm\")\n",
    "            return  \n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r_2 = r2_score(y_test, y_pred)\n",
    "        r_2adj = r2_score(y_test, y_pred,multioutput='variance_weighted')\n",
    "        return mae,mse,r_2,r_2adj,model\n",
    "        \n",
    "    elif task == 'Class':    \n",
    "        if algo == 'decisiontree':\n",
    "            model = DecisionTreeClassifier(max_depth=n) \n",
    "        \n",
    "        elif algo == 'randomforest':\n",
    "            model=RandomForestClassifier(n_estimators=n)\n",
    "            \n",
    "        elif algo == 'knn':\n",
    "            model = KNeighborsClassifier(n_neighbors=n)\n",
    "            \n",
    "        elif algo == 'gboost':\n",
    "            model = GradientBoostingClassifier()\n",
    "            \n",
    "        elif algo == 'adaboost':\n",
    "            model = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "            \n",
    "        elif algo == 'svc':\n",
    "            model = svm.SVC()\n",
    "            \n",
    "        elif algo == 'naive':\n",
    "            model = GaussianNB()\n",
    "            \n",
    "        elif algo == 'xgboost':\n",
    "            model = XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "            \n",
    "        elif algo == 'mlp':\n",
    "            model = MLPClassifier(hidden_layer_sizes=5)\n",
    "        elif algo == 'logit':\n",
    "            model = LogisticRegression()\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid Algorithm\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        c_m =confusion_matrix(y_test, y_pred)\n",
    "        c_r = classification_report(y_test, y_pred)\n",
    "        acc_sc = accuracy_score(y_test, y_pred)\n",
    "        auc_roc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "        logloss = log_loss(y_test, y_pred)       \n",
    "        #Precision\n",
    "        pre_l = precision_score(y_test, y_pred)*100\n",
    "        #Recall\n",
    "        recall_l = recall_score(y_test, y_pred)*100\n",
    "        return c_m,c_r,acc_sc,auc_roc,logloss,model,pre_l,recall_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection algorithms\n",
    "Already Implemented in last assignment features according to the k highest scores.<br>\n",
    "Now Implementing RFFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame with output variable at first location\n",
    "# f_s features to select\n",
    "def feature_selectk(d_f,f_s):\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import f_classif\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x = d_f.iloc[:,1:len(d_f)-1]\n",
    "    y = d_f.iloc[:,:1]\n",
    "    fs = SelectKBest(score_func=f_classif, k=f_s)\n",
    "    # learn relationship from training data\n",
    "    fs.fit(x, y)\n",
    "    # transform train input data\n",
    "    x_fs = fs.transform(x)\n",
    "    return x_fs,y,fs\n",
    "\n",
    "def feature_select_rffs(d_f,reg=True):\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    x = d_f.iloc[:,1:len(d_f.columns)]\n",
    "    y = d_f.iloc[:,:1]\n",
    "    if reg:\n",
    "        sel = SelectFromModel(RandomForestRegressor(n_estimators = 50))\n",
    "    else:\n",
    "        sel = SelectFromModel(RandomForestClassifier(n_estimators = 50))\n",
    "    sel.fit(x, y)\n",
    "    selected_feat= x.columns[(sel.get_support())]\n",
    "    len(selected_feat)\n",
    "    print(selected_feat)\n",
    "    return selected_feat\n",
    "\n",
    "def importantfeat_rffs(d_f,reg=True): \n",
    "    # random forest for feature importance on a regression problem\n",
    "    from sklearn.datasets import make_regression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from matplotlib import pyplot\n",
    "    # define dataset\n",
    "    X = d_f.iloc[:,1:len(d_f.columns)]\n",
    "    y = d_f.iloc[:,:1]\n",
    "    # define the model\n",
    "    model = RandomForestRegressor()\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # get importance\n",
    "    importance = model.feature_importances_\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    # plot feature importance\n",
    "    pyplot.bar([x for x in range(len(importance))], importance)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Cross Validiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfoldtype = KFold , StratifiedKFold\n",
    "def ml_algo_cv(X,y,algo = 'decisiontree',task = 'Reg',n=3,split=10,kftype ='skfold'):\n",
    "    accf = 0\n",
    "    aucf = 0\n",
    "    pref = 0\n",
    "    recallf = 0\n",
    "    i= 0\n",
    "    if kftype == 'skfold':\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=None, shuffle=False)\n",
    "    elif kftype == 'kfold':\n",
    "        kf = KFold(n_splits=split, random_state=None, shuffle=False)\n",
    "        \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        cm,cr,acc,auc,ll,mdl,pre,recall = ml_algo(trainX, testX, trainY, testY,algo = algo ,task = task,n=3)\n",
    "        accf = accf + acc\n",
    "        aucf = aucf + auc\n",
    "        pref = pref + pre\n",
    "        recallf = recallf + recall \n",
    "        i = i + 1\n",
    "    accf = accf/i\n",
    "    aucf = auc/i\n",
    "    pref = pref/i\n",
    "    recallf = recallf/i\n",
    "    return accf,aucf,pref,recallf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsampling minority class and down sampling majority class \n",
    "def class_imbalance(d_f,up=0.75,down=0.75,minor=1,col=None):\n",
    "    df_majority = d_f[d_f[col]!=minor]\n",
    "    df_minority = d_f[d_f[col]==minor]\n",
    "    upc = mt.ceil(len(df_majority) * up)\n",
    "    downc = mt.ceil(len(df_majority) * down)  \n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=upc,    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "    df_majority_under = resample(df_majority, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=downc,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_smp = pd.concat([df_majority_under, df_minority_upsampled])\n",
    "    # Display new class counts\n",
    "    print(df_smp[col].value_counts())\n",
    "    return df_smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07) Other Helper Function\n",
    "Confusion Matrix Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_mat(con_mat,lab = ['x-label','y-label']):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt   \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", ax = ax,cmap=\"YlGnBu\"); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels([lab[0],lab[1]]); ax.yaxis.set_ticklabels([lab[0],lab[1]]);        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 - 12) ML Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FS: Feature Selection\n",
    "# CV: Cross Validiation\n",
    "# CI: CLass Imbalance\n",
    "def ml_com(d_f,colum,algor='decisiontree',fs=True,cv=True,ci=True):\n",
    "    x = col_drop(d_f,colum,typ=1)\n",
    "    y = d_f[colum]\n",
    "    if fs & cv & ci:\n",
    "        df_samp = class_imbalance(d_f,up=0.75,down=0.75,minor=1,col=colum)\n",
    "        fet = feature_select_rffs(df_samp,reg=False)\n",
    "        x = col_drop(df_samp,colum,typ=1)\n",
    "        x = x[fet]\n",
    "        y = df_samp[colum]\n",
    "        acc,auc,pre,recall = ml_algo_cv(x,y,algo = algor,task = 'Class',n=3,split=10,kftype ='skfold')\n",
    "        \n",
    "    elif fs & ~cv & ~ci:\n",
    "        fet = feature_select_rffs(d_f,reg=False)\n",
    "        x = col_drop(d_f,colum,typ=1)\n",
    "        x = x[fet]\n",
    "        y = d_f[colum]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n",
    "        cm,cr,acc,auc,ll,model,pre,recall = ml_algo(X_train, X_test, y_train, y_test,algo = algor,task = 'Class',n=3)\n",
    "    elif ~fs & ~cv & ci:\n",
    "        df_samp = class_imbalance(d_f,up=0.75,down=0.75,minor=1,col=colum)\n",
    "        x = col_drop(df_samp,colum,typ=1)\n",
    "        y = df_samp[colum]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n",
    "        cm,cr,acc,auc,ll,model,pre,recall = ml_algo(X_train, X_test, y_train, y_test,algo = algor,task = 'Class',n=3)\n",
    "    elif ~fs & ~cv & ~ci:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n",
    "        cm,cr,acc,auc,ll,model,pre,recall = ml_algo(X_train, X_test, y_train, y_test,algo = algor,task = 'Class',n=3)\n",
    "\n",
    "    return acc,auc,pre,recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
