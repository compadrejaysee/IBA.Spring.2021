{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import pyplot as plt\n",
    "import missingno as mano\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import pyodbc\n",
    "from scipy.stats import normaltest, shapiro, chisquare, kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: loading data in a dataframe (either CSV or Excel - can be generalized for databases)\n",
    "\n",
    "def load_data(name,typee):\n",
    "    if typee=='excel':\n",
    "        df=pd.read_excel(name)\n",
    "        return df\n",
    "    elif typee=='csv':\n",
    "        df=pd.read_csv(name)\n",
    "        return df\n",
    "    elif typee=='sql':\n",
    "        #query=input('Enter sql query: ')\n",
    "        conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-FI4K9P1;'\n",
    "                      'Database=IBA_23387;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "        cursor = conn.cursor()\n",
    "        df = pd.read_sql_query(name,conn)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2: checking shape, column types, and see the first/last 'n' rows using head/tail (where n is one of the arguments of F2)\n",
    "\n",
    "def df_details(df, n=5):\n",
    "    x='0'\n",
    "    print('Shape: ',df.shape)\n",
    "    print('Column types: ')\n",
    "    print(df.dtypes)\n",
    "    while (x!='H' and x!='T'):\n",
    "        x=input('H/T?')\n",
    "    if x=='H':\n",
    "        return df.head(n)\n",
    "    elif x=='T':\n",
    "        return df.tail(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F3: remove unnecessary/useless columns (based on results of F2 and your background knowledge and the problem to be solved), e.g., identifiers, multiple primary keys, extra KPI like GMROI in sales which is the same for the whole year etc.\n",
    "\n",
    "def remove_columns(df, names): # removes columns mentioned in names from df\n",
    "    for i in names:\n",
    "        df=df.drop([i], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F4: remove rows containing a particular value of a given column, e.g., in smoking_status column, I don't want to consider non-smokers in my ML problem so I remove all these rows.\n",
    "\n",
    "def remove_rows(df, column, names): # removes values from a df column which occur in names\n",
    "    for i in names:\n",
    "        df=df.loc[df[column] != i,:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F5: determine the missing values in the whole dataset\n",
    "\n",
    "def missing_count(df):\n",
    "    a=pd.DataFrame()\n",
    "    a['Missing percentage']=(df.isna().sum()/df.shape[0])*100\n",
    "    a['Missing sum']=df.isna().sum()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F6: analyze missing values of one or more columns using mano module\n",
    "\n",
    "def mano_analysis(*args):\n",
    "    if len(args)==1:   # if only one argummet is passed, it shows tha mano analysis of all the columns in the dataframe\n",
    "        df=args[0]\n",
    "        mano.bar(df)\n",
    "        mano.matrix(df)\n",
    "        mano.heatmap(df)\n",
    "    elif len(args)==2:   # if 2 arguments are passed which includes df as first argumemnt and list of columns as another,\n",
    "        df=args[0]       # then mano analysis of selected columns are shown\n",
    "        names=args[1]\n",
    "        df=df.loc[:,args[1]]\n",
    "        mano.bar(df)\n",
    "        mano.matrix(df)\n",
    "        mano.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F7: cater for missing values (input the column with missing value, and the method through which you want to cater for the missing values)\n",
    "\n",
    "def fill_mv(df,column,method):\n",
    "    if method=='mean':\n",
    "        df[column]=df[column].fillna(df[column].mean(),inplace=True)\n",
    "    elif method=='median':\n",
    "        df[column]=df[column].fillna(df[column].median(),inplace=True)\n",
    "    elif method=='mode':\n",
    "        df[column]=df[column].fillna(df[column].mode(),inplace=True)\n",
    "    elif method=='value':\n",
    "        x=input('Enter value: ')\n",
    "        df[column]=df[column].fillna(x,inplace=True)\n",
    "        \n",
    "    elif method=='linear interpolatiom':\n",
    "        x=input('forward or backward interpolation? ')\n",
    "        if x=='forward':\n",
    "            df[column] = df[column].interpolate(method ='linear', limit_direction ='forward')\n",
    "        elif x=='backward':\n",
    "            df[column] = df[column].interpolate(method ='linear', limit_direction ='backward')\n",
    "            \n",
    "    elif method=='KNN' or method=='knn':\n",
    "        from sklearn.impute import KNNImputer\n",
    "        c=int(input('How many columns do you want to impute? '))\n",
    "        for i in range(c-1):\n",
    "            y=[column]\n",
    "            x=input('Enter column: ')\n",
    "            y.append(x)\n",
    "            \n",
    "            \n",
    "            \n",
    "        n=int(input('Enter number of neighbors: '))\n",
    "        imputer = KNNImputer(n_neighbors=n)\n",
    "        df1=df.copy(deep=True)\n",
    "        df1[y] = imputer.fit_transform(df1[y])\n",
    "        \n",
    "        \n",
    "        a=input('knn plot column x: ')\n",
    "        b=input('knn plot column y: ')\n",
    "        \n",
    "        nulls=df[a].isna() + df[b].isna()\n",
    "        \n",
    "        df1.plot(x=a,y=b,kind='scatter',alpha=0.5,c=nulls,cmap='rainbow')\n",
    "        #df=df1.copy(deep=True)\n",
    "        return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F8: Function for numerical data analysis - includes histogram, boxplot, qqplot, describe, and statistical tests for normality\n",
    "\n",
    "def numerical_analysis(df,x):\n",
    "    print(df[x].describe())\n",
    "    print('')\n",
    "    \n",
    "    print('According to skew and kurtosis test: ')\n",
    "    Skew=skew(df[x])\n",
    "    Kurtosis=kurtosis(df[x])\n",
    "    print('Skew = ',Skew)\n",
    "    print('Kurtosis = ',Kurtosis)\n",
    "    stat, p=normaltest(df[x])\n",
    "    print('stat= ',stat)\n",
    "    print('p= ',p)\n",
    "    if p<0.05:\n",
    "        print('Probably not Normal')\n",
    "    else:\n",
    "        print('Probably Normal')\n",
    "        \n",
    "    print('')\n",
    "    print('According to Shapiro-Wilk test:')\n",
    "    stat,p=shapiro(df[x])\n",
    "    print('stat= ',stat)\n",
    "    print('p= ',p)\n",
    "    if p<0.05:\n",
    "        print('Probably not Normal')\n",
    "    else:\n",
    "        print('Probably Normal')\n",
    "        \n",
    "    print('')\n",
    "    print('According to Chi-Squared normality test: ')\n",
    "    stat, p=chisquare(df[x])\n",
    "    print('stat= ',stat)\n",
    "    print('p= ',p)\n",
    "    if p<0.05:\n",
    "        print('Probably not Normal')\n",
    "    else:\n",
    "        print('Probably Normal')\n",
    "        \n",
    "    print('')\n",
    "    print('according to Kolmogorov-Smirnov test: ')\n",
    "    stat,p=kstest(df[x],'norm')\n",
    "    print('stat= ',stat)\n",
    "    print('p= ',p)\n",
    "    if p<0.05:\n",
    "        print('Probably not Normal')\n",
    "    else:\n",
    "        print('Probably Normal')\n",
    "    \n",
    "    \n",
    "    \n",
    "    sns.distplot(df[x])\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.boxplot(df[x])\n",
    "    fig = sm.qqplot(df[x], line='45')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F9: Function for categorical data analysis - includes value counts, and bar charts\n",
    "\n",
    "def categorical_count(df,x):  # prints value_counts of categorical columns and also plots the bar chart against the count\n",
    "    print(df[x].value_counts())\n",
    "    \n",
    "    aa=pd.DataFrame(df[x].value_counts())\n",
    "    aa=aa.rename(columns={x:'COUNT'})\n",
    "    aa[x]=aa.index\n",
    "    plt.bar(aa[x],aa['COUNT'])\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "def categorical_analysis(df,x,KPI,aggfunc):  # plots bar chart of KPI against a categorical column. For eg price against\n",
    "    z=pd.pivot_table(df,index=x,values=KPI,aggfunc=aggfunc)  # product type\n",
    "    z[x]=z.index\n",
    "    plt.bar(z[x],z[KPI])\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(KPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F10: Function to change the type of any column (input col name and the type you want)\n",
    "\n",
    "def to_numeric(df,columns):\n",
    "    for i in columns:\n",
    "        df[i]=pd.to_numeric(df[i])\n",
    "    return df[i]\n",
    "def to_str(df,columns):\n",
    "    for i in columns:\n",
    "        df[i]=df[i].astype(str)\n",
    "    return df[i]\n",
    "def to_datetime(df, columns):\n",
    "    for i in columns:\n",
    "        df[i]=pd.to_datetime(df[i])\n",
    "    return df[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F11: Function to change the discretizations of a particular categorical column, e.g., rename the values, remove space between value names etc\n",
    "\n",
    "def strip(df, column): # removes leading and trailing white spaces. Also removes excess white spaces between words\n",
    "    import re\n",
    "    df[column]=df[column].str.strip()\n",
    "    df[column] = df[column].replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "def discretize(df,column,value1, value2): # replaces value1 with value2.\n",
    "    df[column].replace(value1, value2, inplace=True) # value1 could be just one string value or list of string values\n",
    "    return df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F12: Function for date analysis - extract year, month etc., subtract dates etc. (this function cannot be specified exactly so just add what you believe are the basic things)\n",
    "\n",
    "# import datetime module for this function\n",
    "def date_extract(df,column): \n",
    "    x=input('Would you like to extract year(Y),month(M),day(D),week(W),hour(H),minute(min) or second(S)?: ')\n",
    "    if (x=='Y') or (x=='y'):\n",
    "        return df[column].dt.year\n",
    "    elif (x=='M') or (x=='m'):\n",
    "        return df[column].dt.month\n",
    "    elif (x=='D') or (x=='d'):\n",
    "        return df[column].dt.day\n",
    "    elif (x=='W') or (x=='w'):\n",
    "        return df[column].dt.week\n",
    "    elif (x=='H') or (x=='h'):\n",
    "        return df[column].dt.hour\n",
    "    elif (x=='min'):\n",
    "        return df[column].dt.minute\n",
    "    else:\n",
    "        return df[column].dt.second\n",
    "\n",
    "# import timedelta module for following function\n",
    "def date_algebra(df, column, days, op):\n",
    "    if op=='sum':\n",
    "        return (df[column]+timedelta(days=days))\n",
    "    elif op=='subtract':\n",
    "        return (df[column]-timedelta(days=days))\n",
    "    else:\n",
    "        print('op should be either sum or subtract')\n",
    "        \n",
    "def duration(df, column1,column2): # return duration between two date columns in days\n",
    "    d=df[column2]-df[column1]\n",
    "    x=input('Would you like duration in years(Y),months(M),days(D),weeks(W),hours(H),minutes(min) or seconds(S)?: ')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F13: function to make a deep copy of a dataframe\n",
    "\n",
    "def df_copy(df):\n",
    "    return df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F14: function to encode categorical into numerical (label, ordinal, or onehot)\n",
    "\n",
    "def encode(df, column, encoder, ordinal_dict={}):\n",
    "    if encoder=='label':\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        df[column]=label_encoder.fit_transform(df[[column]])\n",
    "        return df\n",
    "    elif (encoder=='one hot') or (encoder=='onehot'):\n",
    "        #prefix=input('What prefix do you want for your column?: ')\n",
    "        df=pd.get_dummies(df, columns=column)\n",
    "        return df\n",
    "    elif encoder=='ordinal':\n",
    "        df[column]=df[column].map(ordinal_dict) # {'Male':1,'Female':0}\n",
    "        return df\n",
    "    else:\n",
    "        print('Encoder argument should be either label, ordinal or one hot/onehot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F15: function to split dataframe into X (predictors) and y (label), apply standard scaling on X, apply the desired ML algorithm and output the results:\n",
    "\n",
    "def ml(df,label,algo,classification=True):\n",
    "    oversample=input('Do you want to oversample your training data? (0/1): ')\n",
    "    if algo=='KNN':\n",
    "        n=int(input('Enter number of neighbors: '))\n",
    "    P=df.drop([label],axis=1)\n",
    "    L=df[label]\n",
    "    scaler=StandardScaler()\n",
    "    for i in P.columns:\n",
    "        P[i]=scaler.fit_transform(P[[i]])\n",
    "    dict_classifier={'Logistic Regression':LogisticRegression(),'Decision Tree':DecisionTreeClassifier(),'KNN':KNeighborsClassifier(n_neighbors=n),\n",
    "                     'Naive Bayes':GaussianNB(),'Random Forrest':RandomForestClassifier(),'Gradient Boosting':GradientBoostingClassifier()}\n",
    "    dict_regressor={'Linear Regression':LinearRegression(),'Decision Tree':DecisionTreeRegressor(),'KNN':KNeighborsRegressor(n_neighbors=n),\n",
    "                    'Random Forest':RandomForestRegressor(),'Gradient Boosting':GradientBoostingRegressor()}\n",
    "\n",
    "    ts=float(input('What do you want your test size to be? ')) #0.2\n",
    "    x_train, x_test, y_train, y_test = train_test_split(P,L,test_size=ts,random_state=42)\n",
    "\n",
    "    if classification ==True:\n",
    "        if oversample=='1':\n",
    "            from imblearn.over_sampling import RandomOverSampler\n",
    "            sample = RandomOverSampler(sampling_strategy=0.5)\n",
    "            x_train, y_train=sample.fit_resample(x_train,y_train)\n",
    "        classifier=dict_classifier[algo]\n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred=classifier.predict(x_test)\n",
    "    \n",
    "        print('Classification report: ')\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        print('Accuracy(%): ')\n",
    "        print(accuracy_score(y_test,y_pred)*100)\n",
    "        print('Precision(%): ')\n",
    "        print(precision_score(y_test,y_pred)*100)\n",
    "        print('F1 score: ')\n",
    "        print(f1_score(y_test,y_pred))\n",
    "        print('Logistic loss: ')\n",
    "        print(log_loss(y_test,y_pred))\n",
    "    \n",
    "        confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "        sns.heatmap(confusion_matrix, annot=True)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        regressor=dict_regressor[algo]\n",
    "        regressor.fit(x_train, y_train)\n",
    "        y_pred=regressor.predict(x_test)\n",
    "        \n",
    "        plt.scatter(y_test, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "    \n",
    "        print('Mean Absolute Error: ')\n",
    "        print(mean_absolute_error(y_test,y_pred))\n",
    "        print('Mean Squared Error: ')\n",
    "        print(mean_squared_error(y_test,y_pred))\n",
    "        print('R squared error: ')\n",
    "        print(r2_score(y_test,y_pred))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate_plot(df, label, classification=True, oversample=False):\n",
    "    P=df.drop([label],axis=1)\n",
    "    L=df[label]\n",
    "    scaler=StandardScaler()\n",
    "    for i in P.columns:\n",
    "        P[i]=scaler.fit_transform(P[[i]])\n",
    "    \n",
    "    ts=float(input('What do you want your test size to be? ')) #0.2\n",
    "    x_train, x_test, y_train, y_test = train_test_split(P,L,test_size=ts,random_state=42)\n",
    "    if oversample==True:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        sample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        x_train, y_train=sample.fit_resample(x_train,y_train)\n",
    "    error=[]\n",
    "    if classification==True:\n",
    "        for i in range(1,35):\n",
    "            classifier=KNeighborsClassifier(n_neighbors=i)\n",
    "            classifier.fit(x_train, y_train)\n",
    "            y_pred=classifier.predict(x_test)\n",
    "            error.append(np.mean(y_pred!=y_test))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')\n",
    "            \n",
    "    else:\n",
    "        for i in range(1,35):\n",
    "            regressor=KNeighborsRegressor(n_neighbors=i)\n",
    "            regressor.fit(x_train, y_train)\n",
    "            y_pred=regressor.predict(x_test)\n",
    "            error.append(np.mean(y_pred!=y_test))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following with no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F16: Function to apply ANOVA and output results\n",
    "\n",
    "def anova_test(df):\n",
    "    x=input('Enter the KPI: ')\n",
    "    y=input('Enter the categorical column: ')\n",
    "    model = ols(x + ' ~ C(Q('+y+'))', data=df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    print (\"\\nAnova => \"+x+\" - \"+y)\n",
    "    display(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F17: Function to generate correlation heatmaps\n",
    "\n",
    "def corr_heatmap(df):\n",
    "    corrmatrix = df.corr()\n",
    "    f, axis = plt.subplots(figsize =(15, 10)) \n",
    "    sns.heatmap(corrmatrix, ax = axis, linewidths = 0.2,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F18: Function to generate scatter plot\n",
    "\n",
    "def scatter(df, column1, column2):\n",
    "    plt.scatter(df[column1], df[column2], c='green')\n",
    "    plt.xlabel(column1)\n",
    "    plt.ylabel(column2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
