{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Loading data in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filetype):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    valid = {'sql','csv','excel'}\n",
    "    if filetype not in valid:\n",
    "        raise ValueError(\"filetype must be one of %r.\" % valid)\n",
    "    if filetype=='csv':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_csv(path)\n",
    "    elif filetype=='excel':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_excel(path)\n",
    "    else:\n",
    "        query=input('Enter sql query')\n",
    "        #connect_params=input('Enter connection parameters')\n",
    "        conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-AIS9287\\SQLEXPRESS;'\n",
    "                      'Database=IBA_23371;'# Parameters can be altered accordingly\n",
    "                      'Trusted_Connection=yes;')\n",
    "        cursor = conn.cursor()\n",
    "        df = pd.read_sql_query(query,conn)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(df,head,n):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print('The shape is :',df.shape)\n",
    "    print('The column data types are: \\n')\n",
    "    print(df.dtypes)\n",
    "    print('\\n')\n",
    "    valid = {'head','tail'}\n",
    "    if head not in valid:\n",
    "        raise ValueError(\"Either head or tail\")\n",
    "    if head=='head':\n",
    "        print('The first '+str(n)+' rows are :')\n",
    "        display(df.head(n))\n",
    "    else:\n",
    "        print('The last '+str(n)+' rows are :')\n",
    "        display(df.tail(n))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F3  Remove unnecessary/useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    df.drop(columns=cols,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F4 Remove rows containing a particular value of a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df,column,value):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df.drop(df.loc[df[column]==value].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F5 Determine the missing values in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_details(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n",
    "    b['Missing value count']=df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    return b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F6 Analyze missing values of one or more columns using mano module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_analysis(df,*cols):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import missingno as mn\n",
    "    x=[]\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    df2=df[x]\n",
    "    chart_type=input('Enter the type of chart required')\n",
    "    if chart_type=='bar':\n",
    "        mn.bar(df2)\n",
    "    elif chart_type=='matrix':\n",
    "        mn.matrix(df2)\n",
    "    elif chart_type=='heatmap':\n",
    "        mn.heatmap(df2)\n",
    "    elif chart_type=='dendrogram':\n",
    "        mn.dendrogram(df2)\n",
    "    else:\n",
    "        raise ValueError('Chart type must be on of bar,matrix,heatmap or dendrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F7 Cater for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mv(df,column,method):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if method=='mean':\n",
    "        df[column].fillna(df[column].mean(),inplace=True)\n",
    "    elif method=='median':\n",
    "        df[column].fillna(df[column].median(),inplace=True)\n",
    "    elif method=='mode':\n",
    "        df[column].fillna(df[column].mode()[0],inplace=True)\n",
    "    elif method=='value':\n",
    "        x=input('Enter value to replace null values')\n",
    "        df[column].fillna(x,inplace=True)\n",
    "    elif method=='interpolation':\n",
    "        x=input('forward or backward interpolation?')\n",
    "        if x=='forward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='forward')\n",
    "        elif x=='backward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='backward')\n",
    "        else:\n",
    "            raise ValueError('Select on from forward or backward')\n",
    "    elif method=='KNN':\n",
    "        from sklearn.impute import KNNImputer\n",
    "        neighbors=int(input('Enter number of number of neighbors'))\n",
    "        df2=df.copy()\n",
    "        imputer = KNNImputer (n_neighbors=neighbors)\n",
    "        y=[]\n",
    "        n=int(input('Enter the number of columns to fit KNN imputer'))\n",
    "        for i in range(n):\n",
    "            x=input('Enter name of column')\n",
    "            y.append(x)\n",
    "        y.append(column)\n",
    "        df[y] = imputer.fit_transform(df[y])\n",
    "        x_axis=input('x axis label')\n",
    "        y_axis=input('y axis label')\n",
    "        nulls=df2[x_axis].isna()+df2[y_axis].isna()\n",
    "        df.plot(x=x_axis,y=y_axis,kind='scatter',alpha=0.5,c=nulls,cmap='rainbow')\n",
    "        plt.show()\n",
    "    elif method=='group_mode':        \n",
    "        x=input('Enter name of second column')\n",
    "        df2=df[[column,x]]\n",
    "        df2[column]=df3.groupby(x).transform(lambda group:group.fillna(group.mode()[0]) )\n",
    "        df[column]=df2[column]\n",
    "    elif method=='drop': \n",
    "        df.dropna(axis=0, inplace=True)\n",
    "    else:\n",
    "        raise ValueError('method must be one of mean,median,mode,value,interpolation,KNN,group_mode or drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F8 Function for numerical data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    import pylab as py\n",
    "    sns.distplot(df[column])\n",
    "    plt.show()\n",
    "    sns.boxplot(df[column])\n",
    "    plt.show()\n",
    "    sm.qqplot(df[column], line ='s') \n",
    "    py.show()\n",
    "    display(df[column].describe())\n",
    "    from scipy.stats import skew\n",
    "    from scipy.stats import kurtosis\n",
    "    Skew=skew(df[column])\n",
    "    Kurtosis=kurtosis(df[column])\n",
    "    print('Skew is',Skew)\n",
    "    print('Kurtosis is',Kurtosis)\n",
    "    if Skew>-2 and Skew<2 and Kurtosis>-7 and Kurtosis<7:\n",
    "        print('Can be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    else:\n",
    "        print('Cannot be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    print('\\n')\n",
    "    print('Shapiro Wilk Test for Normality')\n",
    "    from scipy.stats import shapiro\n",
    "    stat,p=shapiro(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to shapiro test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to shapiro test')\n",
    "    print('\\n')\n",
    "    print('Anderson Test for Normality')\n",
    "    from scipy.stats import anderson\n",
    "    result=anderson(df[column])\n",
    "    print('stat=%.3f'%(result.statistic))\n",
    "    for i in range(len(result.critical_values)):\n",
    "        sig_level,cric_value=result.significance_level[i],result.critical_values[i]\n",
    "        if result.statistic<cric_value:\n",
    "            print(f\"Probably Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "        else:\n",
    "            print(f\"Probably Not Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "    print('\\n')\n",
    "    print('Chi square Normality test')\n",
    "    from scipy.stats import chisquare\n",
    "    stat,p=chisquare(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to chisquare test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to chisquare test')\n",
    "    print('\\n')\n",
    "    print('Lilliefors Test for Normality')\n",
    "    from statsmodels.stats.diagnostic import lilliefors\n",
    "    stat,p=lilliefors(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to lilliefors test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to lilliefors test')\n",
    "    print('\\n')\n",
    "    print('Kolmogorov-Smirnov test')\n",
    "    from scipy.stats import kstest\n",
    "    stat,p=kstest(df[column],'norm')\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to Kolmogorov-Smirnov test')\n",
    "    else:\n",
    "        print('Not normally distributed according to Kolmogorov-Smirnov test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F9 Function for categorical data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(df[column].value_counts())\n",
    "    plt.subplots(figsize=(12,6))\n",
    "    sns.countplot(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F10 Function to change the type of any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df,column,t):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if t=='integer':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='integer')\n",
    "    elif t=='float':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='float')\n",
    "    elif t=='datetime':\n",
    "        df[column]=pd.to_datetime(df[column])\n",
    "    elif t=='object':\n",
    "        df[column]=df[column].astype('object')\n",
    "    else:\n",
    "        raise ValueError('Must be one of integer,float,datetime,object')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F11 Function to change the discretizations of a particular catergorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_descr(df,column,value=None,new_value=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    x=input('Remove white space? Y or N')\n",
    "    if x=='Y':\n",
    "        df[column]=df[column].str.replace(' ','')\n",
    "    elif x=='N':\n",
    "        df[column]=df[column].replace(value,new_value)\n",
    "    else:\n",
    "        raise ValueError('Y or N only')\n",
    "    y=input('change column name? Y or N')\n",
    "    if y=='Y':\n",
    "        w=input('Remove with space? Y or N')\n",
    "        if w=='N':\n",
    "            z=input('Enter new column name')\n",
    "            df.rename(columns={column:z},inplace=True)\n",
    "        elif w=='Y':\n",
    "            for i in df.columns:\n",
    "                \n",
    "                c=i.replace(' ','')\n",
    "                df.rename(columns={i:c},inplace=True)\n",
    "                #df.rename(columns=lambda x: x.strip())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F12 Function for data analysis - extract year, month etc., subtract dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_function(df,column,subtract=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df['year']=df[column].dt.year\n",
    "    df['month']=df[column].dt.month\n",
    "    if subtract==True:\n",
    "        x=input('Enter 2nd column name')\n",
    "        df['date_diff']=df[column]-df[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F13 Function to make a deep copy of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_copy(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df2=df.copy(deep=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F14 Function to encode categorical into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    encode_type=input('Enter encode type')\n",
    "    if encode_type=='label':\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le=LabelEncoder()\n",
    "        for i in cols:\n",
    "            df[i]=le.fit_transform(df[[i]])\n",
    "        return df\n",
    "    elif encode_type=='ordinal':\n",
    "        for i in cols:\n",
    "\n",
    "            ord_dict={}\n",
    "            print(i)\n",
    "            n=int(input(\"enter a number of unique values present in the column: \"))\n",
    "            for j in range(n):\n",
    "                key=input('Unique value for the column')\n",
    "                value=int(input('Enter ordinal number for the value'))\n",
    "                ord_dict[key]=value\n",
    "            x=df[i].map(ord_dict)\n",
    "            df[i]=x\n",
    "        return df\n",
    "    elif encode_type=='one-hot':\n",
    "        df2=pd.get_dummies(df,columns=cols)\n",
    "        \n",
    "        return df2\n",
    "    else:\n",
    "        raise Error('Invalid encode type, select one from label,ordinal,one-hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F15 Function to split dataframe into X (predictors) and y (label), apply standard scaling on X, apply the desired ML algorithm and output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(df,label,algorithm,classification=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    algorithms={'Regression','KNN','Naive Bayes','Decision Tree','Random Forest','Gradient Boosting'}\n",
    "    if algorithm not in algorithms:\n",
    "        raise ValueError(\"Algorithm must be one of %r.\" % valid)\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import tree\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    dict_classifiers = {\"Regression\":LogisticRegression(),\"KNN\": KNeighborsClassifier(),\"Decision Tree\": tree.DecisionTreeClassifier(),\"Naive Bayes\": GaussianNB(), \"Random Forest\": RandomForestClassifier(),\"Gradient Boosting\": GradientBoostingClassifier()}\n",
    "    dict_regression={\"Regression\":LinearRegression(),\"KNN\": KNeighborsRegressor(),\"Decision Tree\": tree.DecisionTreeRegressor(), \"Random Forest\": RandomForestRegressor(),\"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        classifier=dict_classifiers[algorithm]\n",
    "        classifier.fit(xtrain,ytrain)\n",
    "        y_pred=classifier.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(classifier)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        if algorithm=='Naive Bayes':\n",
    "            raise Error('Regression cannot be done with Naive Bayes')\n",
    "        else:\n",
    "            regressor=dict_regression[algorithm]\n",
    "            regressor.fit(xtrain,ytrain)\n",
    "            y_pred=regressor.predict(xtest)\n",
    "            print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "            print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "            print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "            r2=metrics.r2_score(ytest,y_pred)\n",
    "            n=len(xtrain)\n",
    "            k=len(x.columns)\n",
    "            r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "            print('The adjusted R Squared Error is',r2_adj)\n",
    "            plt.scatter(ytest, y_pred, c = 'green') \n",
    "            plt.xlabel(\"True Value\") \n",
    "            plt.ylabel(\"Predicted value\") \n",
    "            plt.title(\"True value vs predicted value\") \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_oversample_model(df,label,algorithm,classification=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    algorithms={'Regression','KNN','Naive Bayes','Decision Tree','Random Forest','Gradient Boosting'}\n",
    "    if algorithm not in algorithms:\n",
    "        raise ValueError(\"Algorithm must be one of %r.\" % valid)\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import tree\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    dict_classifiers = {\"Regression\":LogisticRegression(),\"KNN\": KNeighborsClassifier(),\"Decision Tree\": tree.DecisionTreeClassifier(),\"Naive Bayes\": GaussianNB(), \"Random Forest\": RandomForestClassifier(),\"Gradient Boosting\": GradientBoostingClassifier()}\n",
    "    dict_regression={\"Regression\":LinearRegression(),\"KNN\": KNeighborsRegressor(),\"Decision Tree\": tree.DecisionTreeRegressor(), \"Random Forest\": RandomForestRegressor(),\"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #xtrain , xtest , ytrain, ytest =train_test_split(x_res,y_res,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        classifier=dict_classifiers[algorithm]\n",
    "        classifier.fit(x_res, y_res)\n",
    "        y_pred=classifier.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(classifier)\n",
    "        visualizer.fit(x_res, y_res)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        if algorithm=='Naive Bayes':\n",
    "            raise Error('Regression cannot be done with Naive Bayes')\n",
    "        else:\n",
    "            regressor=dict_regression[algorithm]\n",
    "            regressor.fit(x_res, y_res)\n",
    "            y_pred=regressor.predict(xtest)\n",
    "            print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "            print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "            print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "            r2=metrics.r2_score(ytest,y_pred)\n",
    "            n=len(xtrain)\n",
    "            k=len(x.columns)\n",
    "            r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "            print('The adjusted R Squared Error is',r2_adj)\n",
    "            plt.scatter(ytest, y_pred, c = 'green') \n",
    "            plt.xlabel(\"True Value\") \n",
    "            plt.ylabel(\"Predicted value\") \n",
    "            plt.title(\"True value vs predicted value\") \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_with_n(df,label,n=5,classification=True,over_sample=False,sampling_strategy=0.5):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==False:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        xtrain=x_res\n",
    "        ytrain=y_res\n",
    "    \n",
    "    if classification==True:\n",
    "        knn=KNeighborsClassifier(n_neighbors=n)\n",
    "        knn.fit(xtrain,ytrain)\n",
    "        y_pred=knn.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(knn)\n",
    "        visualizer.fit(xtrain,ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        knn=KNeighborsRegressor(n_neighbors=n)\n",
    "        regressor.fit(xtrain, ytrain)\n",
    "        y_pred=regressor.predict(xtest)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_plot(df,label,classification=True,over_sample=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==False:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        xtrain=x_res\n",
    "        ytrain=y_res\n",
    "    if classification==True:\n",
    "\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsClassifier(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')\n",
    "    else:\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsRegressor(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F16 Function to apply ANOVA and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova(df,continous,categorical):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    lm=ols(continous+'~'+categorical,data=df).fit()\n",
    "    table=sm.stats.anova_lm(lm)\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F17 Function to generate correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    corr=df.corr()\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.subplots(figsize=(12,12))\n",
    "    sns.heatmap(corr,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df,column1,column2):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.subplots(figsize=(12,12))\n",
    "    sns.scatterplot(df[column1],df[column2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dependency(data,f1,f2,alpha):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import scipy.stats as s\n",
    "    ov=pd.crosstab(data[f1],data[f2])\n",
    "    b=s.chi2_contingency(ov)\n",
    "    chi2_statistic=b[0]\n",
    "    p_value=b[1]\n",
    "    dof=b[2]\n",
    "    critical_value=s.chi2.ppf(q=1-alpha, df=dof)\n",
    "    print('Significance level: ',alpha)\n",
    "    print('Degree of Freedom: ',dof)\n",
    "    print('chi-square statistic:',chi2_statistic)\n",
    "    print('critical_value:',critical_value)\n",
    "    print('p-value:',p_value)\n",
    "    if chi2_statistic>=critical_value:\n",
    "        print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "    else:\n",
    "        print(\"Retain H0,There is no relationship between 2 categorical variables\")\n",
    "    if p_value<=alpha:\n",
    "        print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "    else:\n",
    "        print(\"Retain H0,There is no relationship between 2 categorical variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
