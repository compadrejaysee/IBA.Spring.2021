{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filetype):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    valid = {'sql','csv','excel'}\n",
    "    if filetype not in valid:\n",
    "        raise ValueError(\"filetype must be one of %r.\" % valid)\n",
    "    if filetype=='csv':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_csv(path)\n",
    "    elif filetype=='excel':\n",
    "        path=input('Enter path of file')\n",
    "        df=pd.read_excel(path)\n",
    "    else:\n",
    "        query=input('Enter sql query')\n",
    "        #connect_params=input('Enter connection parameters')\n",
    "        conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-AIS9287\\SQLEXPRESS;'\n",
    "                      'Database=ML_PROJECT;'# Parameters can be altered accordingly\n",
    "                      'Trusted_Connection=yes;')\n",
    "        cursor = conn.cursor()\n",
    "        df = pd.read_sql_query(query,conn)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(df,head,n):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print('The shape is :',df.shape)\n",
    "    print('The column data types are: \\n')\n",
    "    print(df.dtypes)\n",
    "    print('\\n')\n",
    "    valid = {'head','tail'}\n",
    "    if head not in valid:\n",
    "        raise ValueError(\"Either head or tail\")\n",
    "    if head=='head':\n",
    "        print('The first '+str(n)+' rows are :')\n",
    "        display(df.head(n))\n",
    "    else:\n",
    "        print('The last '+str(n)+' rows are :')\n",
    "        display(df.tail(n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_details(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n",
    "    b['Missing value count']=df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    return b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(group1,group2,related=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    from scipy import stats\n",
    "    if related==False:\n",
    "        stat,p=stats.ttest_ind(group1,group2)\n",
    "        if p<0.05:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error does not exist')\n",
    "        else:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error exists')\n",
    "    else:\n",
    "        stat,p=stats.ttest_rel(group1,group2)\n",
    "        if p<0.05:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error does not exist')\n",
    "        else:\n",
    "            print('P value is:',p)\n",
    "            print('Sampling error exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df,*cols):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pyodbc\n",
    "    from scipy.stats import chi2_contingency\n",
    "    x=[]\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    target=input('Enter target variable')\n",
    "    for i in x:\n",
    "        print(i)\n",
    "        data=pd.crosstab(df[i],df[target])\n",
    "        stat, p, dof, expected = chi2_contingency(data)\n",
    "\n",
    "        # interpret p-value\n",
    "        alpha = 0.05\n",
    "        print(\"p value is \" + str(p))\n",
    "        if p <= alpha:\n",
    "            print('Dependent (reject H0)')\n",
    "        else:\n",
    "            print('Independent (H0 holds true)')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova(df,continous,categorical):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    lm=ols(continous+'~'+categorical,data=df).fit()\n",
    "    table=sm.stats.anova_lm(lm)\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    corr=df.corr()\n",
    "    plt.subplots(figsize=(12,12))\n",
    "    sns.heatmap(corr,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(df,*cols):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    import pylab as py\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    sm.qqplot(df[column], line ='s') \n",
    "    py.show()\n",
    "    from scipy.stats import skew\n",
    "    from scipy.stats import kurtosis\n",
    "    method=input('Enter normality test')\n",
    "    if method=='Shapiro Wilk Test':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import shapiro\n",
    "            stat,p=shapiro(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to shapiro test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to shapiro test')\n",
    "            print('\\n')\n",
    "    elif method=='Anderson':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import anderson\n",
    "            result=anderson(df[column])\n",
    "            print('stat=%.3f'%(result.statistic))\n",
    "            for i in range(len(result.critical_values)):\n",
    "                sig_level,cric_value=result.significance_level[i],result.critical_values[i]\n",
    "                if result.statistic<cric_value:\n",
    "                    print(f\"Probably Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "                else:\n",
    "                    print(f\"Probably Not Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "            print('\\n')\n",
    "    elif method=='Chi square':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import chisquare\n",
    "            stat,p=chisquare(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to chisquare test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to chisquare test')\n",
    "            print('\\n')\n",
    "    elif method=='Lilliefors':\n",
    "        for column in x:\n",
    "\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from statsmodels.stats.diagnostic import lilliefors\n",
    "            stat,p=lilliefors(df[column])\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to lilliefors test')\n",
    "            else:\n",
    "                print('Not Normally distributed according to lilliefors test')\n",
    "            print('\\n')\n",
    "    elif method=='Kolmogorov':\n",
    "        for column in x:\n",
    "            print(column)\n",
    "            sm.qqplot(df[column], line ='s') \n",
    "            py.show()\n",
    "            display(df[column].describe())\n",
    "            from scipy.stats import skew\n",
    "            from scipy.stats import kurtosis\n",
    "            Skew=skew(df[column])\n",
    "            Kurtosis=kurtosis(df[column])\n",
    "            print('Skew is',Skew)\n",
    "            print('Kurtosis is',Kurtosis)\n",
    "            from scipy.stats import kstest\n",
    "            stat,p=kstest(df[column],'norm')\n",
    "            print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "            if p>0.05:\n",
    "                print('Normally distributed according to Kolmogorov-Smirnov test')\n",
    "            else:\n",
    "                print('Not normally distributed according to Kolmogorov-Smirnov test')\n",
    "            print('\\n')\n",
    "    else:\n",
    "        raise ValueError('Method must be one of Shapiro Wilk Test,Anderson,Chi square,Lilliefors,Kolmogorov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mv(df,column,method):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if method=='mean':\n",
    "        df[column].fillna(df[column].mean(),inplace=True)\n",
    "    elif method=='median':\n",
    "        df[column].fillna(df[column].median(),inplace=True)\n",
    "    elif method=='mode':\n",
    "        df[column].fillna(df[column].mode()[0],inplace=True)\n",
    "    elif method=='value':\n",
    "        x=input('Enter value to replace null values')\n",
    "        df[column].fillna(x,inplace=True)\n",
    "    elif method=='interpolation':\n",
    "        x=input('forward or backward interpolation?')\n",
    "        if x=='forward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='forward')\n",
    "        elif x=='backward':\n",
    "            df[column] = df['column'].interpolate(method ='linear', limit_direction ='backward')\n",
    "        else:\n",
    "            raise ValueError('Select on from forward or backward')\n",
    "    elif method=='KNN':\n",
    "        from sklearn.impute import KNNImputer\n",
    "        neighbors=int(input('Enter number of number of neighbors'))\n",
    "        df2=df.copy()\n",
    "        imputer = KNNImputer (n_neighbors=neighbors)\n",
    "        y=[]\n",
    "        n=int(input('Enter the number of columns to fit KNN imputer'))\n",
    "        for i in range(n):\n",
    "            x=input('Enter name of column')\n",
    "            y.append(x)\n",
    "        y.append(column)\n",
    "        df[y] = imputer.fit_transform(df[y])\n",
    "        x_axis=input('x axis label')\n",
    "        y_axis=input('y axis label')\n",
    "        nulls=df2[x_axis].isna()+df2[y_axis].isna()\n",
    "        df.plot(x=x_axis,y=y_axis,kind='scatter',alpha=0.5,c=nulls,cmap='rainbow')\n",
    "        plt.show()\n",
    "    elif method=='group_mode':        \n",
    "        x=input('Enter name of second column')\n",
    "        df2=df[[column,x]]\n",
    "        df2[column]=df3.groupby(x).transform(lambda group:group.fillna(group.mode()[0]) )\n",
    "        df[column]=df2[column]\n",
    "    elif method=='drop': \n",
    "        df.dropna(axis=0, inplace=True)\n",
    "    else:\n",
    "        raise ValueError('method must be one of mean,median,mode,value,interpolation,KNN,group_mode or drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    df.drop(columns=cols,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df,column,value):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df.drop(df.loc[df[column]==value].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_details(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n",
    "    b['Missing value count']=df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    return b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_analysis(df,*cols):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import missingno as mn\n",
    "    x=[]\n",
    "    for i in cols:\n",
    "        x.append(i)\n",
    "    df2=df[x]\n",
    "    chart_type=input('Enter the type of chart required')\n",
    "    if chart_type=='bar':\n",
    "        mn.bar(df2)\n",
    "    elif chart_type=='matrix':\n",
    "        mn.matrix(df2)\n",
    "    elif chart_type=='heatmap':\n",
    "        mn.heatmap(df2)\n",
    "    elif chart_type=='dendrogram':\n",
    "        mn.dendrogram(df2)\n",
    "    else:\n",
    "        raise ValueError('Chart type must be on of bar,matrix,heatmap or dendrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(df[column].value_counts())\n",
    "    plt.subplots(figsize=(12,6))\n",
    "    sns.countplot(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_analysis(df,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    import pylab as py\n",
    "    sns.distplot(df[column])\n",
    "    plt.show()\n",
    "    sns.boxplot(df[column])\n",
    "    plt.show()\n",
    "    sm.qqplot(df[column], line ='s') \n",
    "    py.show()\n",
    "    display(df[column].describe())\n",
    "    from scipy.stats import skew\n",
    "    from scipy.stats import kurtosis\n",
    "    Skew=skew(df[column])\n",
    "    Kurtosis=kurtosis(df[column])\n",
    "    print('Skew is',Skew)\n",
    "    print('Kurtosis is',Kurtosis)\n",
    "    if Skew>-2 and Skew<2 and Kurtosis>-7 and Kurtosis<7:\n",
    "        print('Can be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    else:\n",
    "        print('Cannot be considered normally distributed on the basis of skewness and kurtosis')\n",
    "    print('\\n')\n",
    "    print('Shapiro Wilk Test for Normality')\n",
    "    from scipy.stats import shapiro\n",
    "    stat,p=shapiro(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to shapiro test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to shapiro test')\n",
    "    print('\\n')\n",
    "    print('Anderson Test for Normality')\n",
    "    from scipy.stats import anderson\n",
    "    result=anderson(df[column])\n",
    "    print('stat=%.3f'%(result.statistic))\n",
    "    for i in range(len(result.critical_values)):\n",
    "        sig_level,cric_value=result.significance_level[i],result.critical_values[i]\n",
    "        if result.statistic<cric_value:\n",
    "            print(f\"Probably Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "        else:\n",
    "            print(f\"Probably Not Normally distributed {cric_value} critical value at {sig_level} level of significance\")\n",
    "    print('\\n')\n",
    "    print('Chi square Normality test')\n",
    "    from scipy.stats import chisquare\n",
    "    stat,p=chisquare(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to chisquare test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to chisquare test')\n",
    "    print('\\n')\n",
    "    print('Lilliefors Test for Normality')\n",
    "    from statsmodels.stats.diagnostic import lilliefors\n",
    "    stat,p=lilliefors(df[column])\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to lilliefors test')\n",
    "    else:\n",
    "        print('Not Normally distributed according to lilliefors test')\n",
    "    print('\\n')\n",
    "    print('Kolmogorov-Smirnov test')\n",
    "    from scipy.stats import kstest\n",
    "    stat,p=kstest(df[column],'norm')\n",
    "    print('stat=%.3f,p=%.3f\\n'%(stat,p))\n",
    "    if p>0.05:\n",
    "        print('Normally distributed according to Kolmogorov-Smirnov test')\n",
    "    else:\n",
    "        print('Not normally distributed according to Kolmogorov-Smirnov test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df,column,t):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    if t=='integer':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='integer')\n",
    "    elif t=='float':\n",
    "        df[column]=pd.to_numeric(df[column],downcast='float')\n",
    "    elif t=='datetime':\n",
    "        df[column]=pd.to_datetime(df[column])\n",
    "    elif t=='object':\n",
    "        df[column]=df[column].astype('object')\n",
    "    else:\n",
    "        raise ValueError('Must be one of integer,float,datetime,object')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_descr(df,column,value=None,new_value=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    x=input('Remove white space? Y or N')\n",
    "    if x=='Y':\n",
    "        df[column]=df[column].str.replace(' ','')\n",
    "    elif x=='N':\n",
    "        df[column]=df[column].replace(value,new_value)\n",
    "    else:\n",
    "        raise ValueError('Y or N only')\n",
    "    y=input('change column name? Y or N')\n",
    "    if y=='Y':\n",
    "        w=input('Remove with space? Y or N')\n",
    "        if w=='N':\n",
    "            z=input('Enter new column name')\n",
    "            df.rename(columns={column:z},inplace=True)\n",
    "        elif w=='Y':\n",
    "            for i in df.columns:\n",
    "                \n",
    "                c=i.replace(' ','')\n",
    "                df.rename(columns={i:c},inplace=True)\n",
    "                #df.rename(columns=lambda x: x.strip())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_copy(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    df2=df.copy(deep=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df,*columns):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cols=[]\n",
    "    for i in columns:\n",
    "        cols.append(i)\n",
    "    encode_type=input('Enter encode type')\n",
    "    if encode_type=='label':\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le=LabelEncoder()\n",
    "        for i in cols:\n",
    "            df[i]=le.fit_transform(df[[i]])\n",
    "        return df\n",
    "    elif encode_type=='ordinal':\n",
    "        for i in cols:\n",
    "\n",
    "            ord_dict={}\n",
    "            print(i)\n",
    "            n=int(input(\"enter a number of unique values present in the column: \"))\n",
    "            for j in range(n):\n",
    "                key=input('Unique value for the column')\n",
    "                value=int(input('Enter ordinal number for the value'))\n",
    "                ord_dict[key]=value\n",
    "            x=df[i].map(ord_dict)\n",
    "            df[i]=x\n",
    "        return df\n",
    "    elif encode_type=='one-hot':\n",
    "        df2=pd.get_dummies(df,columns=cols)\n",
    "        \n",
    "        return df2\n",
    "    else:\n",
    "        raise Error('Invalid encode type, select one from label,ordinal,one-hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = LogisticRegression()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def KNN(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = KNeighborsClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def GadientBoosting(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = GradientBoostingClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def AdaBoost(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = AdaBoostClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        if svm_type == \"Linear\":\n",
    "            clf = svm.LinearSVC()\n",
    "        else:\n",
    "            clf = svm.SVC()\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    accuracy=accuracy_score(ytest, y_pred)*100\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    AUC=auc(fpr, tpr)*100\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    precision=precision_score(ytest, y_pred)*100\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    recall=recall_score(ytest,y_pred)*100\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    f1=f1_score(ytest,y_pred)*100\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    log=log_loss(ytest,y_pred)\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    class_report=classification_report(ytest,y_pred)\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "    return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "\n",
    "\n",
    "def DecisionTree(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = DecisionTreeClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def RandomForest(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = RandomForestClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def NaiveBayes(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = GaussianNB()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def MultiLayerPerceptron(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = MLPClassifier(hidden_layer_sizes=5)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def XgBoost(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        clf  = XGBClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "\n",
    "def LightGbm(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    \n",
    "    if not clf:\n",
    "        clf  = LGBMClassifier()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = LinearRegression()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def RandomForestReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = RandomForestRegressor(n_estimators=100)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def PolynomialReg(xtrain, xtest, ytrain, ytest,degree=3, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    X_poly = poly.fit_transform(xtrain)\n",
    "    poly.fit(X_poly, ytrain)\n",
    "    if not clf:\n",
    "        clf = LinearRegression() \n",
    "    clf.fit(X_poly, ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def SupportVectorRegression(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = SVR(kernel=\"rbf\")\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def DecisionTreeReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = DecisionTreeRegressor()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def GradientBoostingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = GradientBoostingRegressor()\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def AdaBooostReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    if not clf:\n",
    "        clf  = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "\n",
    "def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    lr = LinearRegression()\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    sv = SVR(kernel=\"rbf\")\n",
    "    dt = DecisionTreeRegressor()\n",
    "    gb = GradientBoostingRegressor()\n",
    "    ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    if not clf:\n",
    "        clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "    clf.fit(xtrain , ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "    print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "    print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('The RMSE is ',rmse)\n",
    "    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "    r2=metrics.r2_score(ytest,y_pred)\n",
    "    n=len(xtrain)\n",
    "    k=len(x.columns)\n",
    "    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "    print('The adjusted R Squared Error is',r2_adj)\n",
    "    plt.scatter(ytest, y_pred, c = 'green') \n",
    "    plt.xlabel(\"True Value\") \n",
    "    plt.ylabel(\"Predicted value\") \n",
    "    plt.title(\"True value vs predicted value\") \n",
    "    plt.show()\n",
    "    return mse,mae,rmse,r2,r2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_kfold(x, y, split=10, random=None, shuffle=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold \n",
    "    kf = KFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_repeated_kf(x, y, split=10, random=None, repeat=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    kf = RepeatedKFold(n_splits=split, random_state=random, n_repeats=repeat)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest\n",
    "\n",
    "def cross_valid_strat_shuffle_kf(x, y, split=10, random=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=split, random_state=random)\n",
    "    for train_index, test_index in sss.split(x, y):\n",
    "        xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield xtrain,ytrain,xtest,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    if regression:\n",
    "        clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "    else:\n",
    "        clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    #validationmetrics(clf,testX,testY)\n",
    "    res = pd.Series(clf.feature_importances_, index=df.columns.values).sort_values(ascending=False)*100\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample(df,label,ratio=0.5):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    return x_res,y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df,label,ratio=0.5):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    return x_res,y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_plot(df,label,classification=True,over_sample=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    ss=StandardScaler()\n",
    "    for i in x.columns:\n",
    "        x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==False:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        xtrain=x_res\n",
    "        ytrain=y_res\n",
    "    if classification==True:\n",
    "\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsClassifier(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')\n",
    "    else:\n",
    "        error=[]\n",
    "        for j in range(1,35):\n",
    "            knn=KNeighborsRegressor(n_neighbors=j)\n",
    "            knn.fit(xtrain,ytrain)\n",
    "            pred_i=knn.predict(xtest)\n",
    "            error.append(np.mean(pred_i!=ytest))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(1,35),error,color='red',marker='o',markersize=10)\n",
    "        plt.title('Error rate K value')\n",
    "        plt.xlabel('K value')\n",
    "        plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(df,label,classification=True,min_max=False,svm_type = \"Linear\",kernel='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC(kernel='poly')\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "        AUC=metrics.auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_without_CV_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS(df,label,threshold=5,classification=True,min_max=False,svm_type='Linear',kernel='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        print('false positive rate',fpr)\n",
    "        print('true positive rate',tpr)\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_CV(df,label,split=10,classification=True,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC(kernel='poly')\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    \n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xr , ytrain, yr =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , xtest , ycv, ytest =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(xtrain,ytrain)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with CV and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS(df,label,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xr , ytrain, yr =train_test_split(x2,y,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , xtest , ycv, ytest =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms with oversampling without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_os(df,label,ratio=0.5,classification=True,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    \n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "               # print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithms with undersampling without FS,CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_us(df,label,ratio=0.5,classification=True,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "               # print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(xtrain)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with oversampling and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS_os(df,label,ratio=0.5,threshold=5,classification=True,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "               # print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_RFS_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(x_res)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with undersampling and FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_RFS_us(df,label,ratio=0.5,threshold=5,classification=True,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "        print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "        print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "        print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "        print('The adjusted R Squared Error is',r2_adj)\n",
    "        plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    \n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(x_res,y_res)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "               # print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(x_res,y_res)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(x_res, y_res)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_RFS_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            else:\n",
    "                \n",
    "                regressor.fit(x_res,y_res)\n",
    "                y_pred=regressor.predict(xtest)\n",
    "                print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "                print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "                print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "                rmse = math.sqrt(mse)\n",
    "                r2=metrics.r2_score(ytest,y_pred)\n",
    "                rmse = math.sqrt(mse)\n",
    "                mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                n=len(x_res)\n",
    "                k=len(x.columns)\n",
    "                r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                print('The adjusted R Squared Error is',r2_adj)\n",
    "                plt.scatter(ytest, y_pred, c = 'green') \n",
    "                plt.xlabel(\"True Value\") \n",
    "                plt.ylabel(\"Predicted value\") \n",
    "                plt.title(\"True value vs predicted value\") \n",
    "                plt.show()\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = mae\n",
    "                df_results.loc[count,'Mean Squared Error'] = mse\n",
    "                df_results.loc[count,'RMSE'] = rmse\n",
    "                df_results.loc[count,'R squared Error'] = r2\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = r2_adj\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ml algorithm with oversampling and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y,ratio=0.5, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_OS(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)    \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_OS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomOverSampler\n",
    "                    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomOverSampler\n",
    "                    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    regressor.fit(x_res,y_res)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm with undersampling and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y,ratio=0.5, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_US(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)   \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_US.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomUnderSampler\n",
    "                    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(x_res, xtest, y_res, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain , ytrain,xtest, ytest in cross_valid_method(x,y,split=split):\n",
    "                    from imblearn.over_sampling import RandomUnderSampler\n",
    "                    oversample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "                    x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "                    regressor.fit(x_res,y_res)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithm with oversampling,FS and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS_OS(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "        \n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = oversample.fit_resample(xtrain, ytrain)    \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_OS_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.item():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain, ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithm with undersampling,FS and CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "def fit_model_CV_FS_US(df,label,ratio=0.5,split=10,classification=True,cross_valid_method=cross_valid_stratified_kf,threshold=5,min_max=False,svm_type='Linear',kernal='poly'):\n",
    "    def cross_valid_stratified_kf(x, y, split=10, random=None, shuffle=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        kf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "        for train_index, test_index in kf.split(x, y):\n",
    "            xtrain, xtest = x.iloc[train_index], x.iloc[test_index] \n",
    "            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "            yield xtrain,ytrain,xtest,ytest\n",
    "    def RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        if regression:\n",
    "            clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "        else:\n",
    "            clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        #validationmetrics(clf,testX,testY)\n",
    "        res = pd.Series(clf.feature_importances_, index=xtrain.columns.values).sort_values(ascending=False)*100\n",
    "        print(res)\n",
    "        return res\n",
    "    def VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.svm import SVR\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "        from sklearn import metrics\n",
    "        import math\n",
    "        lr = LinearRegression()\n",
    "        rf = RandomForestRegressor(n_estimators=100)\n",
    "        sv = SVR(kernel=\"rbf\")\n",
    "        dt = DecisionTreeRegressor()\n",
    "        gb = GradientBoostingRegressor()\n",
    "        ab = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "        if not clf:\n",
    "            clf = VotingRegressor([('rf', rf), ('dt', dt), ('gb', gb), ('ab', ab)])\n",
    "        clf.fit(xtrain , ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "       # print('Mean Absolute Error is ',metrics.mean_absolute_error(ytest, y_pred))\n",
    "       # print('Mean Squared Error is ',metrics.mean_squared_error(ytest, y_pred))\n",
    "       # print('The R squared Error is ',metrics.r2_score(ytest,y_pred))\n",
    "        rmse = math.sqrt(mse)\n",
    "       # print('The RMSE is ',rmse)\n",
    "        mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "        mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "        r2=metrics.r2_score(ytest,y_pred)\n",
    "        n=len(xtrain)\n",
    "        k=len(x.columns)\n",
    "        r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "       # print('The adjusted R Squared Error is',r2_adj)\n",
    "        '''plt.scatter(ytest, y_pred, c = 'green') \n",
    "        plt.xlabel(\"True Value\") \n",
    "        plt.ylabel(\"Predicted value\") \n",
    "        plt.title(\"True value vs predicted value\") \n",
    "        plt.show()'''\n",
    "        return mse,mae,rmse,r2,r2_adj\n",
    "    def SVM(xtrain, xtest, ytrain, ytest, svmtype=\"Linear\", verbose=True, clf=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn import svm\n",
    "        from sklearn.metrics import auc \n",
    "        from sklearn.metrics import log_loss\n",
    "        from matplotlib import pyplot\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        if not clf:\n",
    "            if svm_type == \"Linear\":\n",
    "                clf = svm.LinearSVC()\n",
    "            else:\n",
    "                clf = svm.SVC()\n",
    "        clf.fit(xtrain,ytrain)\n",
    "        y_pred=clf.predict(xtest)\n",
    "        #print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "        accuracy=accuracy_score(ytest, y_pred)*100\n",
    "        fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "        #print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "        AUC=auc(fpr, tpr)*100\n",
    "        #print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "        precision=precision_score(ytest, y_pred)*100\n",
    "        #print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "        recall=recall_score(ytest,y_pred)*100\n",
    "        #print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "        f1=f1_score(ytest,y_pred)*100\n",
    "        #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "        log=log_loss(ytest,y_pred)\n",
    "        #print(algorithm + '\\n'+classification_report(ytest,y_pred))\n",
    "        class_report=classification_report(ytest,y_pred)\n",
    "        #print(confusion_matrix(ytest, y_pred))\n",
    "        '''visualizer = ConfusionMatrix(clf)\n",
    "        visualizer.fit(xtrain, ytrain)  \n",
    "        visualizer.score(xtest,ytest)\n",
    "        g = visualizer.poof()\n",
    "        pyplot.plot(fpr, tpr, marker='.')\n",
    "        pyplot.xlabel('False Positive Rate')\n",
    "        pyplot.ylabel('True Positive Rate')\n",
    "        pyplot.show()'''\n",
    "        return accuracy,AUC,precision,recall,f1,log,class_report,fpr,tpr\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    #import classification modules\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    # import regression modules\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    import math\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import math\n",
    "    dict_classifiers = {\n",
    "    \n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostClassifier(),\n",
    "    \"SVM\": svm,\n",
    "    'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'light Gradient Boosting':LGBMClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()}\n",
    "    dict_regression={\n",
    "    \n",
    "    \"Lnear Regression\":LinearRegression(),\n",
    "    #\"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "   # \"Naive Bayes\": GaussianNB(), \n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    #\"xgboost\":XGBClassifier(),\n",
    "    \"Ada Boost\":AdaBoostRegressor(random_state=0,n_estimators=1000),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "   # 'MLP Classifier':MLPClassifier(hidden_layer_sizes=5),\n",
    "    'Voting Regression':VotingRegressor,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()}\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.20,random_state=42 )\n",
    "    if classification==True:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=False)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "    else:\n",
    "        res=RFfeatureimportance(df, xtrain, xtest, ytrain, ytest, trees=35, random=None, regression=True)\n",
    "        impftrs = list(res[res > threshold].keys())\n",
    "        print (\"Selected Features =\" + str(impftrs))\n",
    "        x2=x[impftrs]\n",
    "   # for xtrain , xtest , ytrain, ytest in cross_valid_method(x,y,split=split):\n",
    "        \n",
    "    \n",
    "    xtrain , xtest , ytrain, ytest =train_test_split(x2,y,test_size=0.20,random_state=42 )\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    undersample = RandomUnderSampler(sampling_strategy=ratio)\n",
    "    x_res, y_res = undersample.fit_resample(xtrain, ytrain)   \n",
    "    \n",
    "    xtrain , xr , ytrain, yr =train_test_split(x_res,y_res,test_size=0.50,random_state=42 )\n",
    "    \n",
    "    xcv , x_test , ycv, y_test =train_test_split(xr,yr,test_size=0.40,random_state=42 )\n",
    "    print('_____Cross Validation_____')\n",
    "    if classification==True:\n",
    "        #df_results = pd.DataFrame(data=np.zeros(shape=(11,6)), columns = ['classifier','Accuracy','Precision','Recall','f1score','log_loss'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                accuracy=cross_val_score(clf, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(clf, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(clf, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(clf, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(clf, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "            else:\n",
    "                accuracy=cross_val_score(classifier, xcv, ycv,scoring='accuracy', cv=split)\n",
    "                #y_pred=classifier.predict(xtest)\n",
    "                print(\"Mean accuracy  (%): \\n\", (accuracy.mean())*100)\n",
    "                auc=cross_val_score(classifier, xcv, ycv,scoring='roc_auc', cv=split)\n",
    "                print(\"Mean AUC  (%): \\n\",(auc.mean())*100)\n",
    "                precision=cross_val_score(classifier, xcv, ycv,scoring='precision', cv=split)\n",
    "                print(\"Mean Precision: \\n\",(precision.mean())*100)\n",
    "                recall=cross_val_score(classifier, xcv, ycv,scoring='recall', cv=split)\n",
    "                print(\"Mean Recall (%): \\n\",(recall.mean())*100)\n",
    "                f1score=cross_val_score(classifier, xcv, ycv,scoring='f1', cv=split)\n",
    "                print(\"Mean f1 score (%): \\n\",(f1score.mean())*100)\n",
    "                #print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                #print(classification_report(ytest,y_pred))\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                \n",
    "            count+=1\n",
    "        #return df_results\n",
    "    print('______Test_______')\n",
    "    if classification==True:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(11,5)), columns = ['classifier','Precision','Recall','AUC','Accuracy'])\n",
    "        count = 0\n",
    "        for key,classifier in dict_classifiers.items():\n",
    "            print('---------'+key+'------------')\n",
    "            if key=='SVM':\n",
    "                if svm_type == \"Linear\":\n",
    "                    clf = svm.LinearSVC()\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel=kernel)\n",
    "                clf.fit(xtrain,ytrain)\n",
    "                y_pred=clf.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(class_report)\n",
    "                visualizer = ConfusionMatrix(clf)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            else:\n",
    "                classifier.fit(xtrain,ytrain)\n",
    "                y_pred=classifier.predict(xtest)\n",
    "                print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "                fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "                print(\"AUC  (%): \\n\",metrics.auc(fpr, tpr)*100)\n",
    "                print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "                print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "                print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "                print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "                print(classification_report(ytest,y_pred))\n",
    "                accuracy=accuracy_score(ytest, y_pred)*100\n",
    "                AUC=metrics.auc(fpr, tpr)*100\n",
    "                precision=precision_score(ytest, y_pred)*100\n",
    "                recall=recall_score(ytest,y_pred)*100\n",
    "                f1=f1_score(ytest,y_pred)*100\n",
    "                log=log_loss(ytest,y_pred)\n",
    "                df_results.loc[count,'classifier'] = key       \n",
    "                df_results.loc[count,'Precision'] = precision\n",
    "               # df_results.loc[count,'Precision'] = precision\n",
    "                df_results.loc[count,'Recall'] = recall\n",
    "                df_results.loc[count,'AUC']= AUC\n",
    "                df_results.loc[count,'Accuracy'] = accuracy\n",
    "\n",
    "                #df_results.loc[count,'f1score'] = f1\n",
    "                #df_results.loc[count,'log_loss'] = log\n",
    "                #print(confusion_matrix(ytest, y_pred))\n",
    "                visualizer = ConfusionMatrix(classifier)\n",
    "                visualizer.fit(xtrain, ytrain)  \n",
    "                visualizer.score(xtest,ytest)\n",
    "                g = visualizer.poof()\n",
    "                pyplot.plot(fpr, tpr, marker='.')\n",
    "                pyplot.xlabel('False Positive Rate')\n",
    "                pyplot.ylabel('True Positive Rate')\n",
    "                pyplot.show()\n",
    "            count+=1\n",
    "        display(df_results)\n",
    "        df_results.to_csv('E:/Drive/ML1/Untitled Folder/Algorithm_results/ML_algorithm_with_CV_US_FS.csv',index=False)\n",
    "        return df_results\n",
    "    else:\n",
    "        df_results = pd.DataFrame(data=np.zeros(shape=(7,6)), columns = ['Regressor','Mean Absolute Error','Mean Squared Error','RMSE','R squared Error','Adjusted R Squared Error'])\n",
    "        count = 0\n",
    "        for key,regressor in dict_regressors.items():\n",
    "            if key=='Voting Regression':\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain  , ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    mse,mae,rmse,r2,r2_adj=VotingReg(xtrain, xtest, ytrain, ytest, verbose=True, clf=None)\n",
    "                    s.append(mse)\n",
    "                    a.append(mae)\n",
    "                    rm.append(rmse)\n",
    "                    r.append(r2)\n",
    "                    ra.append(r2_adj)\n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "            else:\n",
    "                s=[]\n",
    "                a=[]\n",
    "                rm=[]\n",
    "                r=[]\n",
    "                ra=[]\n",
    "                for xtrain, ytrain,xtest, ytest in cross_valid_method(x2,y,split=split):\n",
    "                    \n",
    "                    regressor.fit(xtrain,ytrain)\n",
    "                    y_pred=regressor.predict(xtest)\n",
    "                    mse=metrics.mean_squared_error(ytest, y_pred)\n",
    "                    s.append(mse)\n",
    "                    mae=metrics.mean_absolute_error(ytest, y_pred)\n",
    "                    a,append(mae)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    rm.append(rmse)\n",
    "                    r2=metrics.r2_score(ytest,y_pred)\n",
    "                    r.append(r2)\n",
    "                    rmse = math.sqrt(mse)\n",
    "                    \n",
    "                    n=len(xtrain)\n",
    "                    k=len(x.columns)\n",
    "                    r2_adj=1-((1-r2)*(n-1)/(n-k-1))\n",
    "                    ra.append(r2_adj)\n",
    "                print('Mean Absolute Error is ',np.mean(a))\n",
    "                print('Mean Squared Error is ',np.mean(s))\n",
    "                print('The R squared Error is ',np.mean(r))\n",
    "                \n",
    "                print('The adjusted R Squared Error is',np.mean(ra))\n",
    "                \n",
    "                df_results.loc[count,'Regressor'] = key       \n",
    "                df_results.loc[count,'Mean Absolute Error'] = np.mean(a)\n",
    "                df_results.loc[count,'Mean Squared Error'] = np.mean(s)\n",
    "                df_results.loc[count,'RMSE'] = np.mean(rm)\n",
    "                df_results.loc[count,'R squared Error'] = np.mean(r)\n",
    "                df_results.loc[count,'Adjusted R Squared Error'] = np.mean(ra)\n",
    "            count+=1\n",
    "        return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(df,label, svmtype=\"SVC\",over_sample=False,under_sample=False,name=None,clf=None,min_max=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import auc \n",
    "    from sklearn.metrics import log_loss\n",
    "    from matplotlib import pyplot\n",
    "    from yellowbrick.classifier import ConfusionMatrix\n",
    "    if not clf:\n",
    "        if svmtype == \"Linear\":\n",
    "            clf = svm.LinearSVC()\n",
    "        else:\n",
    "            clf = svm.SVC(kernel='poly')\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    x=df.drop([label], axis=1)\n",
    "    y=df[label]\n",
    "    if min_max==True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mm = MinMaxScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = mm.fit_transform(x[[i]])\n",
    "    else:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss=StandardScaler()\n",
    "        for i in x.columns:\n",
    "            x[i] = ss.fit_transform(x[[i]])\n",
    "    if over_sample==True:\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "        x_res, y_res = oversample.fit_resample(xtrain, ytrain)\n",
    "        x_train=x_res\n",
    "        y_train=y_res\n",
    "    elif under_sample==True:\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "        x_res, y_res = undersample.fit_resample(xtrain, ytrain)\n",
    "        x_train=x_res\n",
    "        y_train=y_res\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        xtrain , xtest , ytrain, ytest =train_test_split(x,y,test_size=0.50,random_state=42 )\n",
    "\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    y_pred=clf.predict(xtest)\n",
    "    print(\"Accuracy  (%): \\n\", accuracy_score(ytest, y_pred)*100)\n",
    "    accuracy=accuracy_score(ytest, y_pred)*100\n",
    "    fpr , tpr, _ = roc_curve(ytest, y_pred)\n",
    "    print(\"AUC  (%): \\n\",auc(fpr, tpr)*100)\n",
    "    AUC=auc(fpr, tpr)*100\n",
    "    print(\"Precision: \\n\",precision_score(ytest, y_pred)*100)\n",
    "    precision=precision_score(ytest, y_pred)*100\n",
    "    print(\"Recall (%): \\n\",recall_score(ytest, y_pred)*100)\n",
    "    recall=recall_score(ytest,y_pred)*100\n",
    "    print(\"f1 score (%): \\n\",f1_score(ytest, y_pred)*100)\n",
    "    f1=f1_score(ytest,y_pred)*100\n",
    "    print('logistic loss :\\n',log_loss(ytest,y_pred))\n",
    "    log=log_loss(ytest,y_pred)\n",
    "    print(classification_report(ytest,y_pred))\n",
    "    class_report=classification_report(ytest,y_pred)\n",
    "    #print(confusion_matrix(ytest, y_pred))\n",
    "    visualizer = ConfusionMatrix(clf)\n",
    "    visualizer.fit(xtrain, ytrain)  \n",
    "    visualizer.score(xtest,ytest)\n",
    "    g = visualizer.poof()\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.show()\n",
    "    import pickle\n",
    "    filename = name\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return accuracy,AUC,precision,recall,f1,log,class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
